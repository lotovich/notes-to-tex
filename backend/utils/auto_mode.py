"""
Auto-Mode Selector for Notes-to-TeX (–≠–¢–ê–ü 2)

This module automatically determines whether to use 'book' or 'strict' mode
based on analysis of the raw OCR text content.

Logic:
1. Check BLOCKERS (‚Üí BOOK if triggered)
2. If no blockers ‚Üí Analyze content type for STRICT
3. Default: BOOK (safe choice)

Author: AI Assistant
Version: 1.0
"""

import re
from typing import Dict, List, Set


# ========================================
# CONSTANTS - BLOCKERS (‚Üí BOOK)
# ========================================

INFORMAL_ABBREVIATIONS = {
    "ru": [
        # –ë–∞–∑–æ–≤—ã–µ
        "—Ç.–∫.", "—Ç.–µ.", "—Ç.–ø.", "—Ç.–¥.", "–∏ —Ç.–¥.", "–∏ —Ç.–ø.",
        "=> ", "-> ", "~",

        # –ò–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–ª–µ–Ω–≥
        "–∏ —Ç–¥", "–∏—Ç–¥", "–∫–º–∫", "–∫—Å—Ç", "–º–±", "–∏–º—Ö–æ",
        "—Å–ø—Å", "–ø–∂–ª", "–ø–∂", "–Ω–æ—Ä–º", "–æ–∫",

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ
        "–≤-–æ–±—â–µ–º", "–≤ –æ–±—â–µ–º", "–Ω—É —Ç–∏–ø", "—á–µ—Ç", "—á—ë"
    ],
    "en": [
        # Arrows and symbols
        "=> ", "-> ", "~",

        # Common abbreviations
        "btw", "imo", "fyi", "aka", "asap",
        "etc.", "w/", "w/o", "thx", "pls", "plz",

        # Internet slang
        "lol", "idk", "tbh", "smth", "sth",
        "gonna", "wanna", "kinda", "sorta"
    ]
}

PERSONAL_MARKERS = {
    "ru": [
        # TODO-–ø–æ–¥–æ–±–Ω—ã–µ
        "TODO", "todo", "—Ç—É–¥—É—à–∫–∞",

        # –í–æ—Å–∫–ª–∏—Ü–∞–Ω–∏—è
        "!!!", "???", "!?",

        # –ò–º–ø–µ—Ä–∞—Ç–∏–≤—ã (–±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –∑–∞–º–µ—Ç–æ–∫)
        "–í–ê–ñ–ù–û", "–≤–∞–∂–Ω–æ", "–í–ê–ñ–ù–û:",
        "–ù–ï –ó–ê–ë–´–¢–¨", "–Ω–µ –∑–∞–±—ã—Ç—å",
        "—Ä–∞–∑–æ–±—Ä–∞—Ç—å—Å—è", "–ø–æ–Ω—è—Ç—å —ç—Ç–æ",
        "—Å–ø—Ä–æ—Å–∏—Ç—å", "—É—Ç–æ—á–Ω–∏—Ç—å",
        "–ø—Ä–æ–≤–µ—Ä–∏—Ç—å", "–ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å",
        "REMINDER", "–Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ",

        # –û—Ü–µ–Ω–∫–∏ (–Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ)
        "–∫—Ä—É—Ç–æ", "—Ñ–∏–≥–Ω—è", "–Ω–µ–ø–æ–Ω—è—Ç–Ω–æ"
    ],
    "en": [
        # TODO-like
        "TODO", "todo", "FIXME", "NOTE",

        # Exclamations
        "!!!", "???", "!?",

        # Imperatives (more specific to notes)
        "IMPORTANT", "important", "NOTE",
        "DON'T FORGET", "don't forget",
        "figure out", "understand this",
        "ask about", "clarify",
        "check", "review",
        "REMINDER", "reminder",

        # Evaluations
        "cool", "bad", "unclear"
    ]
}

LECTURE_META_PATTERNS = {
    "ru": [
        r"–õ–µ–∫—Ü–∏—è\s+\d+",           # "–õ–µ–∫—Ü–∏—è 5"
        r"–ü–∞—Ä–∞\s+\d+",             # "–ü–∞—Ä–∞ 3"
        r"–ó–∞–Ω—è—Ç–∏–µ\s+\d+",          # "–ó–∞–Ω—è—Ç–∏–µ 7"
        r"–°–µ–º–∏–Ω–∞—Ä\s+\d+",          # "–°–µ–º–∏–Ω–∞—Ä 2"
        r"–ö–æ–Ω—Å–ø–µ–∫—Ç\s+–æ—Ç",          # "–ö–æ–Ω—Å–ø–µ–∫—Ç –æ—Ç 12.03"
        # Date ONLY in lecture context
        r"(?:–õ–µ–∫—Ü–∏—è|–ü–∞—Ä–∞|–ó–∞–Ω—è—Ç–∏–µ|–°–µ–º–∏–Ω–∞—Ä|–ü—Ä–µ–ø–æ–¥|–ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å|–ö—É—Ä—Å).*?\d{2}\.\d{2}\.\d{4}",
        r"–ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å:",
        r"–ü—Ä–µ–ø–æ–¥:",
    ],
    "en": [
        r"Lecture\s+\d+",          # "Lecture 5"
        r"Class\s+\d+",            # "Class 3"
        r"Seminar\s+\d+",          # "Seminar 2"
        r"Notes\s+from",           # "Notes from 03/12"
        # Date ONLY in lecture context
        r"(?:Lecture|Class|Seminar|Professor|Instructor).*?(?:[A-Z][a-z]+\s+\d{1,2},\s+\d{4}|\d{2}/\d{2}/\d{4})",
        r"Professor:",
        r"Instructor:",
    ]
}

CASUAL_PHRASES = {
    "ru": [
        "–∫–æ—Ä–æ—á–µ", "–≤ –æ–±—â–µ–º", "—Ç–∏–ø–∞", "—Ç–∏–ø–æ",
        "–Ω—É –≤–æ—Ç", "–Ω—É", "–±–ª–∏–Ω", "–∫–æ—Ä–æ—á",
        "–∫–∞–∫ –±—ã", "—á–µ—Ç", "—á—ë—Ç",
        "–∫–∞—Ä–æ—á", "–≤–æ–æ–±—â–µ", "–≤–æ–±—â–µ",
        "–≤–∞—â–µ", "–ø—Ä–æ—Å—Ç", "–ø—Ä–æ—Å—Ç–æ"
    ],
    "en": [
        "basically", "kinda", "sorta",
        "you know", "whatever", "stuff",
        "gonna", "wanna"
    ]
}

# –ü–æ—Ä–æ–≥–∏ –¥–ª—è –±–ª–æ–∫–µ—Ä–æ–≤
ABBREVIATION_THRESHOLD = 2
PERSONAL_MARKER_THRESHOLD = 2
LECTURE_META_THRESHOLD = 1
CASUAL_PHRASE_THRESHOLD = 2

# –£—Ä–æ–≤–Ω–∏ confidence
LECTURE_META_CONFIDENCE = 0.9


# ========================================
# TEXT EXTRACTION UTILITIES
# ========================================

def extract_text_from_meta(meta: dict) -> str:
    """
    Extract text from meta.json blocks for classification.
    Used as fallback when ocr_raw.txt is empty/failed.

    Args:
        meta: Meta dictionary with 'blocks' key

    Returns:
        Concatenated text from all paragraph and section blocks
    """
    if not isinstance(meta, dict):
        return ""

    blocks = meta.get("blocks", [])
    if not blocks:
        return ""

    text_parts = []
    for block in blocks:
        if not isinstance(block, dict):
            continue

        block_type = block.get("type")

        if block_type in ("section", "paragraph"):
            text = block.get("text", "")
            if text:
                text_parts.append(text)
        elif block_type == "list":
            items = block.get("items", [])
            if items:
                text_parts.extend(items)

    return "\n\n".join(text_parts)


# ========================================
# HELPER FUNCTIONS - BLOCKER COUNTERS
# ========================================

def count_abbreviations(text: str, lang: str) -> int:
    """Count unique abbreviations found in text."""
    if lang not in INFORMAL_ABBREVIATIONS:
        return 0

    abbr_list = INFORMAL_ABBREVIATIONS[lang]
    found = set()

    for abbr in abbr_list:
        # For short abbreviations, require word boundaries to avoid false positives
        if len(abbr.strip()) <= 3:
            import re
            # Create pattern with word boundaries for short abbreviations
            pattern = r'\b' + re.escape(abbr.strip()) + r'\b'
            if re.search(pattern, text, re.I):
                found.add(abbr)
        else:
            # For longer phrases, use simple substring search
            if abbr in text:
                found.add(abbr)

    return len(found)


def count_personal_markers(text: str, lang: str) -> int:
    """Count unique personal markers found in text."""
    if lang not in PERSONAL_MARKERS:
        return 0

    marker_list = PERSONAL_MARKERS[lang]
    found = set()

    for marker in marker_list:
        if marker in text:
            found.add(marker)

    return len(found)


def count_lecture_metadata(text: str, lang: str) -> int:
    """Count lecture metadata patterns."""
    if lang not in LECTURE_META_PATTERNS:
        return 0

    patterns = LECTURE_META_PATTERNS[lang]
    count = 0

    for pattern in patterns:
        if re.search(pattern, text, re.I | re.M):
            count += 1

    return count


def count_casual_phrases(text: str, lang: str) -> int:
    """Count unique casual phrases found in text."""
    if lang not in CASUAL_PHRASES:
        return 0

    phrase_list = CASUAL_PHRASES[lang]
    found = set()

    for phrase in phrase_list:
        if phrase in text.lower():
            found.add(phrase)

    return len(found)


# ========================================
# STRICT CONTENT DETECTORS
# ========================================

def detect_quality_latex(text: str) -> bool:
    """Detect high-quality LaTeX (already well-formatted)."""

    # Check for LaTeX commands in source
    has_sections = len(re.findall(r'\\section', text)) >= 2
    has_labels = len(re.findall(r'\\label\{', text)) >= 1
    has_refs = len(re.findall(r'\\(?:ref|eqref)\{', text)) >= 1

    # NEW: Check for numbered structure (from rendered PDF)
    has_numbered_sections = len(re.findall(r'^\d+\s+[A-Z]', text, re.M)) >= 2
    has_subsections = len(re.findall(r'^\d+\.\d+\s+[A-Z]', text, re.M)) >= 1
    has_equation_numbers = len(re.findall(r'\(\d+\)', text)) >= 2

    # Check for LaTeX-style equation references
    has_latex_refs = bool(re.search(r'Equation\s+\(\d+\)|equation\s+\(\d+\)', text, re.I))

    # Check for OCR noise
    ocr_noise_patterns = [
        r'\?\?+',
        r'√¢‚Ç¨‚Ñ¢',
        r'\[\[illegible\]\]',
    ]
    has_noise = any(re.search(p, text) for p in ocr_noise_patterns)

    # Decision: LaTeX source OR rendered structured document
    is_latex_source = (has_sections or (has_labels and has_refs)) and not has_noise
    is_rendered_latex = (has_numbered_sections and has_equation_numbers) or \
                        (has_subsections and has_latex_refs)

    return is_latex_source or is_rendered_latex


def detect_formal_essay(text: str, lang: str) -> bool:
    """
    Detect formal essay or article (not study notes).

    Indicators:
    - Long paragraphs (>50 words)
    - Formal connectors
    - No numbered lists like "1)", "2)"
    """
    # 1. Long paragraphs - split by multiple newlines and filter empty
    paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
    long_paragraphs = sum(1 for p in paragraphs if len(p.split()) > 50)

    # 2. Formal connectors
    formal_connectors = {
        "ru": [
            "—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ", "—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º", "–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ",
            "–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –æ—Ç–º–µ—Ç–∏—Ç—å", "–≤ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ",
            "–±–æ–ª–µ–µ —Ç–æ–≥–æ", "–∫—Ä–æ–º–µ —Ç–æ–≥–æ", "–∏–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏"
        ],
        "en": [
            "therefore", "consequently", "as a result",
            "it should be noted", "in conclusion",
            "moreover", "furthermore", "in other words"
        ]
    }

    if lang not in formal_connectors:
        connector_count = 0
    else:
        connector_count = sum(1 for conn in formal_connectors[lang]
                             if conn.lower() in text.lower())

    # 3. No numbered lists
    no_numbered_lists = len(re.findall(r"^\d+\)", text, re.M)) < 3

    # Adaptive threshold based on text length
    text_length = len(text)

    if text_length < 1000:
        # Short essay: 1 long para + 1 connector, OR 2 long paras
        return (long_paragraphs >= 1 and connector_count >= 1) or long_paragraphs >= 2
    else:
        # Long essay: keep requirements
        return (long_paragraphs >= 2 and connector_count >= 1) or connector_count >= 2


def detect_narrative_text(text: str, lang: str) -> bool:
    """
    Detect narrative/literary text (story, essay).

    Uses regex with word boundaries to catch past tense verbs
    even when followed by punctuation.

    CRITICAL: Uses re.UNICODE flag for Cyrillic text support.
    """

    # Past tense markers with word boundaries
    past_tense_markers = {
        "ru": [
            r"\b–±—ã–ª\b", r"\b–±—ã–ª–∞\b", r"\b–±—ã–ª–∏\b", r"\b–±—ã–ª–æ\b",
            r"\b—Å—Ç–∞–ª\b", r"\b—Å—Ç–∞–ª–∞\b", r"\b—Å—Ç–∞–ª–∏\b", r"\b—Å—Ç–∞–ª–æ\b",
            r"\b–ø–æ—à—ë–ª\b", r"\b–ø–æ—à–ª–∞\b", r"\b–ø–æ—à–ª–∏\b",
            r"\b—Å–∫–∞–∑–∞–ª\b", r"\b—Å–∫–∞–∑–∞–ª–∞\b", r"\b—Å–∫–∞–∑–∞–ª–∏\b",
            r"\b–ø—Ä–æ–ª–µ—Ç–∞–ª–∞\b", r"\b–ø—Ä–æ–ª–µ—Ç–∞–ª\b", r"\b–ø—Ä–æ–ª–µ—Ç–µ–ª–∏\b",
            r"\b—Å–º–µ—à–∞–ª–∏—Å—å\b", r"\b—Å–º–µ—à–∞–ª–æ—Å—å\b", r"\b—Å–º–µ—à–∞–ª–∞—Å—å\b",
            r"\b–æ—Å—Ç–∞–≤–∞–ª–∞—Å—å\b", r"\b–æ—Å—Ç–∞–≤–∞–ª—Å—è\b", r"\b–æ—Å—Ç–∞–≤–∞–ª–æ—Å—å\b",
            r"\b–æ—Å—Ç–∞–≤–∞–ª–∏—Å—å\b",
            r"\b–æ—Ç–¥–µ–ª–∏–ª–∏—Å—å\b",
            r"\b–æ—Ç–¥–µ–ª–∏–ª–∞—Å—å\b",
            r"\b—Ä–æ–¥–∏–ª–∞\b",
        ],
        "en": [
            r"\bwas\b", r"\bwere\b", r"\bhad\b",
            r"\bwent\b", r"\bsaid\b", r"\bbecame\b",
            r"\bwalked\b", r"\blooked\b", r"\bthought\b",
            r"\bwrote\b", r"\boffered\b", r"\bargued\b"
        ]
    }

    # Count matches with UNICODE flag for Cyrillic support
    past_count = 0
    for pattern in past_tense_markers.get(lang, []):
        # CRITICAL: re.UNICODE flag is essential for Cyrillic word boundaries!
        if re.search(pattern, text, re.IGNORECASE | re.UNICODE):
            past_count += 1

    # Check for dialogue markers
    has_quotes = ('"' in text or '¬´' in text or '¬ª' in text or
                 text.count("'") >= 4)

    # Decision: ‚â•3 past tense markers OR quotes
    return past_count >= 3 or has_quotes


def detect_formal_math(text: str, lang: str) -> bool:
    """
    Detect formal mathematical proof/theorem.

    Indicators:
    - Math keywords (Theorem, Lemma, Proof)
    - Many formulas
    - Logical connectors
    """
    # 1. Math keywords
    math_keywords = {
        "ru": [
            "–¢–µ–æ—Ä–µ–º–∞", "–õ–µ–º–º–∞", "–î–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ",
            "–°–ª–µ–¥—Å—Ç–≤–∏–µ", "–£—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ",
            "–Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ",
            "—Ç–æ–≥–¥–∞ –∏ —Ç–æ–ª—å–∫–æ —Ç–æ–≥–¥–∞"
        ],
        "en": [
            "Theorem", "Lemma", "Proof",
            "Corollary", "Proposition",
            "if and only if",
            "necessary and sufficient"
        ]
    }

    if lang not in math_keywords:
        keyword_count = 0
    else:
        keyword_count = sum(1 for kw in math_keywords[lang] if kw in text)

    # 2. Formula count
    formula_count = text.count('$') + text.count('\\[') + text.count('\\begin{equation')

    return keyword_count >= 2 and formula_count >= 5


def detect_formulas_only(text: str) -> bool:
    """
    Detect if document is ONLY formulas with minimal text.
    This should trigger BOOK mode (needs explanations).

    Logic:
    - Simple heuristic: count math symbols vs regular words
    - If too many formulas with little explanatory text -> formulas-only
    """
    # Count formula indicators
    dollar_count = text.count('$')
    equation_count = text.count('\\begin{equation')
    display_math_count = text.count('\\[')

    total_formula_indicators = dollar_count + equation_count + display_math_count

    # Count words (rough estimate of explanatory content)
    words = re.findall(r'\b[a-zA-Z–∞-—è—ë]+\b', text, re.I)
    word_count = len(words)

    # If many formula indicators but few words ‚Üí formulas only
    if total_formula_indicators >= 6 and word_count < 20:
        return True

    # Alternative: if the ratio of formula indicators to words is very high
    if word_count > 0 and (total_formula_indicators / word_count) > 0.5:
        return True

    return False


# ========================================
# MAIN CLASSIFICATION FUNCTION
# ========================================

def classify_content_mode(text: str, meta: dict) -> dict:
    """
    Classify content as 'book' or 'strict' mode.

    Args:
        text: Raw OCR text from ocr_raw.txt
        meta: Metadata dict with 'language' field

    Returns:
        {
            "mode": "book" | "strict",
            "confidence": float (0.0-1.0),
            "reasons": [list of reason strings],
            "blocked_by": str | None,  # Which blocker triggered (if any)
            "scores": {
                "abbreviations": int,
                "personal_markers": int,
                "lecture_metadata": int,
                "casual_phrases": int
            }
        }

    Logic:
        1. Check BLOCKERS (‚Üí BOOK if triggered)
        2. If no blockers ‚Üí Analyze content type
        3. Default: BOOK (safe choice)
    """
    lang = meta.get("language", "en")


    reasons = []
    scores = {
        "abbreviations": 0,
        "personal_markers": 0,
        "lecture_metadata": 0,
        "casual_phrases": 0
    }

    # ========================================
    # SPECIAL CASE: FORMULAS-ONLY ‚Üí BOOK (before LaTeX check)
    # ========================================

    if detect_formulas_only(text):
        # Fill scores for statistics
        scores["abbreviations"] = count_abbreviations(text, lang)
        scores["personal_markers"] = count_personal_markers(text, lang)
        scores["lecture_metadata"] = count_lecture_metadata(text, lang)
        scores["casual_phrases"] = count_casual_phrases(text, lang)

        return {
            "mode": "book",
            "confidence": 0.9,
            "reasons": ["Formulas-only document ‚Üí needs explanations"],
            "blocked_by": "formulas_only",
            "scores": scores
        }

    # ========================================
    # PRIORITY CHECK: High-quality LaTeX ‚Üí STRICT immediately
    # ========================================

    if detect_quality_latex(text):
        # Fill scores for statistics
        scores["abbreviations"] = count_abbreviations(text, lang)
        scores["personal_markers"] = count_personal_markers(text, lang)
        scores["lecture_metadata"] = count_lecture_metadata(text, lang)
        scores["casual_phrases"] = count_casual_phrases(text, lang)

        return {
            "mode": "strict",
            "confidence": 0.85,
            "reasons": ["High-quality LaTeX (already well-formatted)"],
            "blocked_by": None,
            "scores": scores
        }

    # ========================================
    # –≠–¢–ê–ü 1: –ü–†–û–í–ï–†–ö–ê –ë–õ–û–ö–ï–†–û–í
    # ========================================

    # –ë–ª–æ–∫–µ—Ä 1: –°–æ–∫—Ä–∞—â–µ–Ω–∏—è
    abbr_count = count_abbreviations(text, lang)
    scores["abbreviations"] = abbr_count

    if abbr_count >= ABBREVIATION_THRESHOLD:
        return {
            "mode": "book",
            "confidence": 1.0,
            "reasons": [f"üö´ BLOCKER: Study abbreviations ({abbr_count})"],
            "blocked_by": "abbreviations",
            "scores": scores
        }

    # –ë–ª–æ–∫–µ—Ä 2: –õ–∏—á–Ω—ã–µ –ø–æ–º–µ—Ç–∫–∏
    personal_count = count_personal_markers(text, lang)
    scores["personal_markers"] = personal_count

    if personal_count >= PERSONAL_MARKER_THRESHOLD:
        return {
            "mode": "book",
            "confidence": 1.0,
            "reasons": [f"üö´ BLOCKER: Personal study markers ({personal_count})"],
            "blocked_by": "personal_notes",
            "scores": scores
        }

    # –ë–ª–æ–∫–µ—Ä 3: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –ª–µ–∫—Ü–∏–∏
    lecture_meta = count_lecture_metadata(text, lang)
    scores["lecture_metadata"] = lecture_meta

    if lecture_meta >= LECTURE_META_THRESHOLD:
        return {
            "mode": "book",
            "confidence": LECTURE_META_CONFIDENCE,
            "reasons": ["üö´ BLOCKER: Lecture metadata"],
            "blocked_by": "lecture_metadata",
            "scores": scores
        }

    # –ë–ª–æ–∫–µ—Ä 4: –†–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–π —Å—Ç–∏–ª—å
    casual_count = count_casual_phrases(text, lang)
    scores["casual_phrases"] = casual_count

    if casual_count >= CASUAL_PHRASE_THRESHOLD:
        return {
            "mode": "book",
            "confidence": 1.0,
            "reasons": [f"üö´ BLOCKER: Casual language ({casual_count})"],
            "blocked_by": "casual_language",
            "scores": scores
        }


    # ========================================
    # –≠–¢–ê–ü 3: –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –¢–ò–ü–ê STRICT –ö–û–ù–¢–ï–ù–¢–ê
    # ========================================

    strict_reasons = []

    if detect_quality_latex(text):
        strict_reasons.append("High-quality LaTeX (already well-formatted)")

    if detect_formal_essay(text, lang):
        strict_reasons.append("Formal essay/article")

    if detect_narrative_text(text, lang):
        strict_reasons.append("Literary/narrative text")

    if detect_formal_math(text, lang):
        strict_reasons.append("Formal mathematical proof")

    # ========================================
    # –≠–¢–ê–ü 4: –§–ò–ù–ê–õ–¨–ù–û–ï –†–ï–®–ï–ù–ò–ï
    # ========================================

    if len(strict_reasons) >= 1:
        # –ï—Å—Ç—å —Ö–æ—Ç—è –±—ã 1 –ø—Ä–∏–∑–Ω–∞–∫ STRICT ‚Üí –≤—ã–±–∏—Ä–∞–µ–º STRICT
        confidence = 0.75 + len(strict_reasons) * 0.05
        confidence = min(confidence, 0.95)

        return {
            "mode": "strict",
            "confidence": round(confidence, 2),
            "reasons": strict_reasons,
            "blocked_by": None,
            "scores": scores
        }

    # ========================================
    # –≠–¢–ê–ü 5: –ü–û –£–ú–û–õ–ß–ê–ù–ò–Æ BOOK (–±–µ–∑–æ–ø–∞—Å–Ω—ã–π –≤—ã–±–æ—Ä)
    # ========================================

    default_reasons = []

    if abbr_count > 0:
        default_reasons.append(f"Has abbreviations ({abbr_count}), but not a blocker")
    if personal_count > 0:
        default_reasons.append(f"Has markers ({personal_count}), but not a blocker")

    if not default_reasons:
        default_reasons.append("Insufficient indicators for strict mode ‚Üí safe choice: book")

    return {
        "mode": "book",
        "confidence": 0.6,
        "reasons": default_reasons,
        "blocked_by": None,
        "scores": scores
    }